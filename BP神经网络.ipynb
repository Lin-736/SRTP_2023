{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ede1562",
   "metadata": {},
   "source": [
    "需要import的package。\n",
    "第一个是python的numpy科学计算库，可以将矩阵向量化，避免显示for循环，大幅提升运算速度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8874b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f063abfe",
   "metadata": {},
   "source": [
    "激活函数是每个隐藏层节点和输出层的输出，输入和权重矩阵相乘，加上初始偏置值后，再作为激活函数自变量，映射后进行输出。\n",
    "\n",
    "relu函数即线性激活函数，定义为：(python中）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6cf8f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义激活函数\n",
    "def relu(x):\n",
    "    return np.maximum(x, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4d4602",
   "metadata": {},
   "source": [
    "除此之外，还有带泄露的relu函数：\n",
    "\n",
    "（本次运算中，带泄露的relu函数并不会用到）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fdc65cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_2(x):\n",
    "   \n",
    "   if x>=0:\n",
    "     \n",
    "     return np.maximum(x, 0)\n",
    "   \n",
    "   if x>=0:\n",
    "     \n",
    "     return np.minimum(0.01*x, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06af9599",
   "metadata": {},
   "source": [
    "除此之外，还有signoid函数和tanh函数，这两个函数都是为了将输出归一到[-1,1]，同时这两个函数还是非线性函数，事实上只有非线性的输出才能进行拟合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2460834b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # 初始化权重和偏置\n",
    "        self.W1 = np.random.randn(input_size, hidden_size)\n",
    "        self.b1 = np.random.randn(hidden_size)\n",
    "        self.W2 = np.random.randn(hidden_size, output_size)\n",
    "        self.b2 = np.random.randn(output_size)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # 前向传播计算\n",
    "        Z1 = np.dot(X, self.W1) + self.b1\n",
    "        A1 = relu(Z1)\n",
    "        Z2 = np.dot(A1, self.W2) + self.b2\n",
    "        return Z2\n",
    "\n",
    "    def train(self, X, y, learning_rate=0.01, max_epochs=1000):\n",
    "        # 训练模型\n",
    "        for epoch in range(max_epochs):\n",
    "            # 前向传播计算\n",
    "            Z1 = np.dot(X, self.W1) + self.b1\n",
    "            A1 = relu(Z1)\n",
    "            Z2 = np.dot(A1, self.W2) + self.b2\n",
    "\n",
    "            # 计算误差\n",
    "            loss = np.mean((Z2 - y) ** 2)\n",
    "\n",
    "            # 反向传播更新权重和偏置\n",
    "            dZ2 = Z2 - y\n",
    "            dW2 = np.dot(A1.T, dZ2)\n",
    "            db2 = np.sum(dZ2, axis=0)\n",
    "            dA1 = np.dot(dZ2, self.W2.T)\n",
    "            dZ1 = dA1 * (Z1 > 0)\n",
    "            dW1 = np.dot(X.T, dZ1)\n",
    "            db1 = np.sum(dZ1, axis=0)\n",
    "\n",
    "            self.W1 -= learning_rate * dW1\n",
    "            self.b1 -= learning_rate * db1\n",
    "            self.W2 -= learning_rate * dW2\n",
    "            self.b2 -= learning_rate * db2\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                print(\"Epoch %d, Loss = %f\" % (epoch, loss))\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6901621a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss = 364990.291207\n",
      "Epoch 100, Loss = 198501.043065\n",
      "Epoch 200, Loss = 140227.100194\n",
      "Epoch 300, Loss = 118843.021054\n",
      "Epoch 400, Loss = 110995.965954\n",
      "Epoch 500, Loss = 108116.427707\n",
      "Epoch 600, Loss = 107059.758624\n",
      "Epoch 700, Loss = 106672.005639\n",
      "Epoch 800, Loss = 106529.716649\n",
      "Epoch 900, Loss = 106477.502591\n",
      "Epoch 1000, Loss = 106458.342234\n",
      "Epoch 1100, Loss = 106451.311191\n",
      "Epoch 1200, Loss = 106448.731095\n",
      "Epoch 1300, Loss = 106447.784308\n",
      "Epoch 1400, Loss = 106447.436877\n",
      "Epoch 1500, Loss = 106447.309385\n",
      "Epoch 1600, Loss = 106447.262601\n",
      "Epoch 1700, Loss = 106447.245433\n",
      "Epoch 1800, Loss = 106447.239133\n",
      "Epoch 1900, Loss = 106447.236821\n",
      "Epoch 2000, Loss = 106447.235973\n",
      "Epoch 2100, Loss = 106447.235661\n",
      "Epoch 2200, Loss = 106447.235547\n",
      "Epoch 2300, Loss = 106447.235505\n",
      "Epoch 2400, Loss = 106447.235490\n",
      "Epoch 2500, Loss = 106447.235484\n",
      "Epoch 2600, Loss = 106447.235482\n",
      "Epoch 2700, Loss = 106447.235481\n",
      "Epoch 2800, Loss = 106447.235481\n",
      "Epoch 2900, Loss = 106447.235481\n",
      "Epoch 3000, Loss = 106447.235481\n",
      "Epoch 3100, Loss = 106447.235481\n",
      "Epoch 3200, Loss = 106447.235481\n",
      "Epoch 3300, Loss = 106447.235481\n",
      "Epoch 3400, Loss = 106447.235481\n",
      "Epoch 3500, Loss = 106447.235481\n",
      "Epoch 3600, Loss = 106447.235481\n",
      "Epoch 3700, Loss = 106447.235481\n",
      "Epoch 3800, Loss = 106447.235481\n",
      "Epoch 3900, Loss = 106447.235481\n",
      "Epoch 4000, Loss = 106447.235481\n",
      "Epoch 4100, Loss = 106447.235481\n",
      "Epoch 4200, Loss = 106447.235481\n",
      "Epoch 4300, Loss = 106447.235481\n",
      "Epoch 4400, Loss = 106447.235481\n",
      "Epoch 4500, Loss = 106447.235481\n",
      "Epoch 4600, Loss = 106447.235481\n",
      "Epoch 4700, Loss = 106447.235481\n",
      "Epoch 4800, Loss = 106447.235481\n",
      "Epoch 4900, Loss = 106447.235481\n",
      "Epoch 5000, Loss = 106447.235481\n",
      "Epoch 5100, Loss = 106447.235481\n",
      "Epoch 5200, Loss = 106447.235481\n",
      "Epoch 5300, Loss = 106447.235481\n",
      "Epoch 5400, Loss = 106447.235481\n",
      "Epoch 5500, Loss = 106447.235481\n",
      "Epoch 5600, Loss = 106447.235481\n",
      "Epoch 5700, Loss = 106447.235481\n",
      "Epoch 5800, Loss = 106447.235481\n",
      "Epoch 5900, Loss = 106447.235481\n",
      "Epoch 6000, Loss = 106447.235481\n",
      "Epoch 6100, Loss = 106447.235481\n",
      "Epoch 6200, Loss = 106447.235481\n",
      "Epoch 6300, Loss = 106447.235481\n",
      "Epoch 6400, Loss = 106447.235481\n",
      "Epoch 6500, Loss = 106447.235481\n",
      "Epoch 6600, Loss = 106447.235481\n",
      "Epoch 6700, Loss = 106447.235481\n",
      "Epoch 6800, Loss = 106447.235481\n",
      "Epoch 6900, Loss = 106447.235481\n",
      "Epoch 7000, Loss = 106447.235481\n",
      "Epoch 7100, Loss = 106447.235481\n",
      "Epoch 7200, Loss = 106447.235481\n",
      "Epoch 7300, Loss = 106447.235481\n",
      "Epoch 7400, Loss = 106447.235481\n",
      "Epoch 7500, Loss = 106447.235481\n",
      "Epoch 7600, Loss = 106447.235481\n",
      "Epoch 7700, Loss = 106447.235481\n",
      "Epoch 7800, Loss = 106447.235481\n",
      "Epoch 7900, Loss = 106447.235481\n",
      "Epoch 8000, Loss = 106447.235481\n",
      "Epoch 8100, Loss = 106447.235481\n",
      "Epoch 8200, Loss = 106447.235481\n",
      "Epoch 8300, Loss = 106447.235481\n",
      "Epoch 8400, Loss = 106447.235481\n",
      "Epoch 8500, Loss = 106447.235481\n",
      "Epoch 8600, Loss = 106447.235481\n",
      "Epoch 8700, Loss = 106447.235481\n",
      "Epoch 8800, Loss = 106447.235481\n",
      "Epoch 8900, Loss = 106447.235481\n",
      "Epoch 9000, Loss = 106447.235481\n",
      "Epoch 9100, Loss = 106447.235481\n",
      "Epoch 9200, Loss = 106447.235481\n",
      "Epoch 9300, Loss = 106447.235481\n",
      "Epoch 9400, Loss = 106447.235481\n",
      "Epoch 9500, Loss = 106447.235481\n",
      "Epoch 9600, Loss = 106447.235481\n",
      "Epoch 9700, Loss = 106447.235481\n",
      "Epoch 9800, Loss = 106447.235481\n",
      "Epoch 9900, Loss = 106447.235481\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.NeuralNetwork at 0x2dec08bc4c0>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建神经网络模型\n",
    "input_size = 4\n",
    "hidden_size = 4\n",
    "output_size = 1\n",
    "model = NeuralNetwork(input_size, hidden_size, output_size)\n",
    "\n",
    "# 五组数据作为训练集X和标签y\n",
    "X_train = np.array([[3, 100, 4, 1], [3, 80, 8, 1], [2, 130, 1, 1], [1, 150, 2, 2], [5, 120, 4, 3]])\n",
    "y_train = np.array([305.427684, 195.427684, 431.945280, 438.591409, 1127.065795]).reshape(-1, 1)\n",
    "\n",
    "# 训练模型\n",
    "model.train(X_train, y_train, learning_rate=0.001, max_epochs=10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4b18cec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[499.6915704]\n",
      " [499.6915704]]\n"
     ]
    }
   ],
   "source": [
    "# 预测房价\n",
    "X_test = np.array([[3, 100, 4, 1], [2, 300, 4, 6]])\n",
    "y_pred = model.forward(X_test)\n",
    "print(y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
